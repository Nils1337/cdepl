#!/bin/bash

readonly __HILBERT_MAX_NODES="112"
readonly __HILBERT_MAX_CPUS_PER_NODE="24"
readonly __HILBERT_MAX_MEM_PER_NODE="106903"
readonly __HILBERT_ETH_BASE_IP="10.28.5."
readonly __HILBERT_HOSTNAME="hpc.rz.uni-duesseldorf.de"

__HILBERT_TOTAL_NODES="0"
__HILBERT_WALLTIME=""
__HILBERT_CPUS_PER_NODE="0"
__HILBERT_MEM_MB_PER_NODE="0"

__HILBERT_USER=""
__HILBERT_PROJECT=""
__HILBERT_JOB_NAME=""

__HILBERT_JOB_ID=""
__HILBERT_NODE_MAPPING=()

declare -gA __HILBERT_DEPENDENCY_MAP
__HILBERT_DEPENDENCY_MAP["java/1.8"]="Java/1.8.0"

cdepl_cluster_node_alloc()
{
	local num_nodes=$1

    if [ "$num_nodes" -gt "$__HILBERT_MAX_NODES" ]; then
        util_log_error_and_exit "[hilbert] Cannot allocate $num_nodes, max: $__HILBERT_MAX_NODES"
    fi

	__HILBERT_TOTAL_NODES="$num_nodes"
}

cdepl_cluster_get_alloc_node_count()
{
	echo "$__HILBERT_TOTAL_NODES"
}

cdepl_cluster_walltime()
{
	local walltime=$1

	__HILBERT_WALLTIME="$walltime"
}

cdepl_cluster_node_excl()
{
	local node=$1

	# TODO reserve exclusively?
    util_log_error_and_exit "Reserve exclusive not implemented for hilbert"
}

cdepl_cluster_node_cpus()
{
	local node=$1
	local cpus=$2

	# I don't think it's possible to allocate a different number of cpus
    # per instance
    # Just use the highest number and allocate that amount on every node
    if [ "$cpus" -gt "$__HILBERT_CPUS_PER_NODE" ] && [ "$cpus" -le "$__HILBERT_MAX_CPUS_PER_NODE" ]; then
        __HILBERT_CPUS_PER_NODE="$cpus"
    fi
}

cdepl_cluster_node_mem()
{
	local node=$1
	local mem=$2

    if [ "$mem" -gt "$__HILBERT_MEM_MB_PER_NODE" ] && [ "$mem" -le "$__HILBERT_MAX_MEM_PER_NODE" ]; then
        __HILBERT_MEM_MB_PER_NODE="$mem"
    fi	
}

cdepl_cluster_node_network()
{
	local node=$1
	local net=$2

	# All nodes are supporting eth and ib
}

cdepl_cluster_resolve_dependency()
{
	local node=$1
	local cmd=$2
	local version=$3

	util_log_debug "[hilbert][$node] Resolve dependency: $cmd/$version"

    local key="$cmd/$version"

    local hilbert_dep=${__HILBERT_DEPENDENCY_MAP[$key]}

    if [ ! "$hilbert_dep" ]; then
        util_log_error_and_exit "[hilbert][$node] Could not resolve dependency $cmd/$version, no mapping available"
    fi

    cdepl_cluster_node_cmd $node "module load $hilbert_dep"
}

cdepl_cluster_resolve_hostname_to_ip()
{
	local hostname=$1

    # Fast method to resolve all hilbertXXX hostnames
    local node_id="$(echo "$hostname" | sed 's/hilbert//')"

	if [ ! "$hilbert_id" ]; then
        echo ""
    else
        echo "$__HILBERT_ETH_BASE_IP" "$hilbert_id"
    fi
}

cdepl_cluster_resolve_node_to_ip()
{
	local node=$1

	cdepl_cluster_resolve_hostname_to_ip "${__HILBERT_NODE_MAPPING[$node]}"
}

cdepl_cluster_node_resolve_node_to_hostname()
{
	local node=$1

	echo "${__HILBERT_NODE_MAPPING[$node]}"
}

cdepl_cluster_login_cmd()
{
	local cmd="$1"

	# -n -f for nohup
	# Use ControlMaster to establish the TCP connection, once and keep it active
	# until all sessions are closed and no further session was established
	# within 60 seconds 
	# Note when using nohup: You must redirect stdin/stdout and not add any
	# command before it using &&, e.g. cd ~ && nohup ...
	# This will hang the session with controlmaster
	ssh -o ControlMaster=auto -o ControlPath=~/.ssh/cdepl_%r@%h:%p.sock -o ControlPersist=60 ${__HILBERT_USER}@${__HILBERT_HOSTNAME} -n -f "$cmd"
}

cdepl_cluster_send_file_to_login()
{
    local source="$1"
    local destination="$2"

    scp "$source" "${__HILBERT_USER}@${__HILBERT_HOSTNAME}:${destination}"
}

cdepl_cluster_node_cmd()
{
	local node=$1
	local cmd="$2"

	if [ "${__HILBERT_NODE_MAPPING[$node]}" = "" ]; then
		util_log_error_and_exit "[hilbert][node $node] Exec node $cmd, node does not exist"
	fi

	# -n -f for nohup
	# Use ControlMaster to establish the TCP connection, once and keep it active
	# until all sessions are closed and no further session was established
	# within 60 seconds 
	# Note when using nohup: You must redirect stdin/stdout and not add any
	# command before it using &&, e.g. cd ~ && nohup ...
	# This will hang the session with controlmaster
	ssh -o ControlMaster=auto -o ControlPath=~/.ssh/cdepl_%r@%h:%p.sock -o ControlPersist=60 ${__HILBERT_USER}@${__HILBERT_HOSTNAME} -n -f "ssh ${__HILBERT_NODE_MAPPING[$node]} \"$cmd\"" 
}

####################################
# "private" callback methods that should not be called by the user

# "constructor"
_cdepl_cluster_on_init()
{
	local cluster_user=$1
    # Required for job system
    local project_name=$2
    # Optional args
    local job_name=$3

	util_log_debug "[hilbert] on_init: user $cluster_user project $project_name"

	__HILBERT_USER="$cluster_user"
    
    if [ ! "$project_name" ]; then
        util_log_error_and_exit "[hilbert] No project name specified on init"
    fi
    
    __HILBERT_PROJECT="$project_name"

    # If no job name specified, default
    if [ "$job_name" ]; then
        __HILBERT_JOB_NAME="$job_name"
    else
        __HILBERT_JOB_NAME="${__HILBERT_PROJECT}_job"
    fi

    # Check if passwordless auth available
	ssh -o PasswordAuthentication=no -o BatchMode=yes ${__HILBERT_USER}@${__HILBERT_HOSTNAME} exit &> /dev/null

	if [ "$?" != "0" ]; then
		util_log_error_and_exit "[hilbert] Can't connect to login node, ensure passwordless auth is set up and the cluster user is valid"
	fi
}

_cdepl_cluster_on_node_setup_finish()
{
	util_log_debug "[local] on_node_setup_finish"

    # Create job script to bootstrap the cluster: allocate nodes, get node mappings

    local tmp_job_script="/tmp/cdepl_hilbert_deploy.job"
    local job_script="${__DEPLOY_CUR_OUT_PATH}/deploy.job"
    local node_mappings_path="${__DEPLOY_CUR_OUT_PATH}/node_mappings"

    local total_cpus=$(($__HILBERT_CPUS_PER_NODE * $__HILBERT_TOTAL_NODES))

    # TODO add parameter for email notification (see pbs doc)

    printf '%s' "#!/bin/bash

#PBS -l select=${__HILBERT_TOTAL_NODES}:ncpus=${total_cpus}:mem=${__HILBERT_MEM_MB_PER_NODE}MB:arch=ivybridge
#PBS -l place=scatter
#PBS -l walltime=${__HILBERT_WALLTIME}
#PBS -r n
#PBS -N $__HILBERT_JOB_NAME
#PBS -A $__HILBERT_PROJECT
#PBS -e ${__DEPLOY_CUR_OUT_PATH}/deploy.stderr
#PBS -o ${__DEPLOY_CUR_OUT_PATH}/deploy.stdout

JOB_WORKING_DIR=\"${__DEPLOY_CUR_OUT_PATH}\"
NODES_COUNT=\"\"
NODES_HOSTNAME=\"\"  
NODE_MAPPINGS=\"\"

echo \"Getting reserved nodes...\"

NODES_COUNT=0
for NODE in \$(cat \$PBS_NODEFILE); do
    NODES_HOSTNAME[\$NODES_COUNT]=\$NODE
    NODES_COUNT=\$[NODES_COUNT + 1]
done

echo \"Total node count: \$NODES_COUNT\"

echo \"Resolving node mappings for deploy conf: \$__DEPLOY_CUR_OUT_PATH\"

i=0
while [ \$i -lt \$NODES_COUNT ]; do
    NODE_MAPPINGS=\"\${NODE_MAPPINGS}\${i} \${NODES_HOSTNAME[i]}\\n\"
    i=\$[i + 1]
done 

printf \"\$NODE_MAPPINGS\" > \"$node_mappings_path\"
sync

# Keep job running to keep allocated nodes active
run_time=\$(echo \"${__HILBERT_WALLTIME}\" | awk -F: '{ print (\$1 * 3600) + (\$2 * 60) + \$3 }')

echo \"Keep job running for \$run_time seconds...\"
sleep \$run_time

echo \"Job finished\"
" > $tmp_job_script

    cdepl_cluster_send_file_to_login "$tmp_job_script" "$job_script" > /dev/null 2>&1

    # Submit the job script and wait for the node mappings to be written to disk

    local qsub_output=$(cdepl_cluster_login_cmd "qsub $job_script")

    if [ "$?" != "0" ]; then
        util_log_error_and_exit "Submitting job script $job_script failed"
    fi

    # Wait for node_mappings file to appear

    __HILBERT_JOB_ID=$(echo $qsub_output | cut -d '.' -f 1)

    util_log_debug "[hilbert] Job submitted $__HILBERT_JOB_ID ($qsub_output), waiting for job to start..."

    local node_mappings=""

    # ... and get the node mappings assigned by the job system

    while true; do
        if [ "$(cdepl_cluster_login_cmd "[ -f ${node_mappings_path} ] && echo 1")" = "1" ]; then
            node_mappings=$(cdepl_cluster_login_cmd "cat $node_mappings_path")
            while read -r line; do
                local id=$(echo "$line" | cut -d ' ' -f 1)
                local hostname=$(echo "$line" | cut -d ' ' -f 2)

                __HILBERT_NODE_MAPPING[$id]="$hostname"
            done <<< "$node_mappings"

            break
        fi

        sleep 1
        echo -n "."
    done

    echo ""

    util_log_debug "[hilbert] Job started, node allocations:\n${node_mappings}"
}

_cdepl_cluster_on_env_setup_finish()
{
	util_log_debug "[hilbert] on_env_setup_finish"
}

_cdepl_cluster_before_deploy()
{
	util_log_debug "[hilbert] before_deploy"
}

_cdepl_cluster_after_deploy()
{
	util_log_debug "[hilbert] after_deploy"
}

_cdepl_cluster_before_cleanup()
{
	util_log_debug "[hilbert] before_cleanup"
}

_cdepl_cluster_after_cleanup()
{
	util_log_debug "[hilbert] after_cleanup"

    util_log_debug "[hilbert] Deployment finished, delete job $__HILBERT_JOB_ID"
    cdepl_cluster_login_cmd "qdel $__HILBERT_JOB_ID"
}
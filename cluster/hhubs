#!/bin/bash

# Heterogeneous hardware, 3 groups
readonly __HHUBS_MAX_NUM_NODES=24
readonly __HHUBS_MAX_NUM_CORES=6
readonly __HHUBS_MAX_RAM=65536

# 4 cores, 32 GB RAM, InfiniBand
readonly __HHUBS_NODE_GROUP_1="node49 node50 node51 node52 node53 node54 node55 node56"
readonly __HHUBS_TOTAL_NODES_GROUP_1=8
readonly __HHUBS_NODE_GROUP_1_MAX_CORES=4
readonly __HHUBS_NODE_GROUP_1_MAX_RAM=32768
readonly __HHUBS_NODE_GROUP_1_MAX_NET="ib"

# 4 cores, 16 GB RAM
readonly __HHUBS_NODE_GROUP_2="node57 node58 node59 node60 node61 node62 node63 node64"
readonly __HHUBS_TOTAL_NODES_GROUP_2=8
readonly __HHUBS_NODE_GROUP_2_MAX_CORES=4
readonly __HHUBS_NODE_GROUP_2_MAX_RAM=16384
readonly __HHUBS_NODE_GROUP_2_MAX_NET="eth"

# 6 cores, 64 GB RAM, InfiniBand
readonly __HHUBS_NODE_GROUP_3="node65 node66 node67 node68 node69 node70 node71 node72"
readonly __HHUBS_TOTAL_NODES_GROUP_3=8
readonly __HHUBS_NODE_GROUP_3_MAX_CORES=6
readonly __HHUBS_NODE_GROUP_3_MAX_RAM=65536
readonly __HHUBS_NODE_GROUP_3_MAX_NET="ib"

__HHUBS_SOLLIPULLI_MODE=0
__HHUBS_SOLLIPULLI_USER=""

__HHUBS_TOTAL_NODES=""

# Init with minimums 0 and Ethernet
__HHUBS_NODES_CPUS=(0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0)
__HHUBS_NODES_MEM=(0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0)
__HHUBS_NODES_NET=("" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "")

__HHUBS_NODE_MAP=""

cdepl_cluster_node_alloc()
{
	local num_nodes=$1

	if [ "$num_nodes" -gt "$__HHUBS_MAX_NUM_NODES" ]; then
		util_log_error_and_exit "[hhubs] Not enough nodes available to allocate $num_nodes nodes, max $__HHUBS_MAX_NUM_NODES nodes"
	fi

	__HHUBS_TOTAL_NODES="$num_nodes"
}

cdepl_cluster_walltime()
{
	local walltime=$1

	# Not supporting walltime, ignore
}

cdepl_cluster_node_excl()
{
	local node=$1

	# Nodes are always exclusive, ignore
}

cdepl_cluster_node_cpus()
{
	local node=$1
	local cpus=$2

	if [ "$cpus" -gt "$__HHUBS_MAX_NUM_CORES" ]; then
		util_log_error_and_exit "[hhubs] No instance with $cpus cpus available, max cpus $__HHUBS_MAX_NUM_CORES"
	fi

	__HHUBS_NODES_CPUS[$node]="$cpus"
}

# mem in MB
cdepl_cluster_node_mem()
{
	local node=$1
	local mem=$2

	if [ "$mem" -gt "$__HHUBS_MAX_RAM" ]; then
		util_log_error_and_exit "[hhubs] No instance with $mem mb memory available, max mem $__HHUBS_MAX_RAM mb"
	fi

	__HHUBS_NODES_MEM[$node]="$mem"
}

cdepl_cluster_node_network()
{
	local node=$1
	local net=$2

	if [ "$net" != "eth" ] && [ "$net" != "ib" ]; then
		util_log_error_and_exit "[hhubs] No instance with network $net available, supported networks: eth, ib"
	fi

	__HHUBS_NODES_NET[$node]="$net"
}

cdepl_cluster_resolve_dependency()
{
	local node=$1
	local cmd=$2
	local version=$3

	if [ "${__HHUBS_NODE_MAP[$node]}" = "" ]; then
		util_log_error_and_exit "[hhubs][$node] Resolve dependency $cmd/$version, remote node does not exist"
	fi

	util_log_debug "[hhubs][$node: ${__HHUBS_NODE_MAP[$node]}]: Resolve dependency: $cmd/$version"

	__cdepl_cluster_hhubs_remote_node_cmd ${__HHUBS_NODE_MAP[$node]} "command -v $cmd" > /dev/null 2>&1

	if [ "$?" != "0" ]; then
		util_log_error_and_exit "[hhubs][$node: ${__HHUBS_NODE_MAP[$node]}] Dependency $cmd/$version does not exist"
	fi
}

# if non resolveable, returns empty string
cdepl_cluster_resolve_hostname_to_ip()
{
	local hostname=$1

	local ip=""

	# TODO don't use dig etc -> just remove the id from the hostname, e.g. node65 -> 65 -> combine: 10.0.0.65

	ip="$(__cdepl_cluster_hhubs_remote_sollipulli_cmd "host $hostname | cut -d ' ' -f 4 | grep -E '[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}'")"

	if [ "$ip" = "" ]; then
		ip="$(__cdepl_cluster_hhubs_remote_sollipulli_cmd "dig $hostname | grep -E '[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}' | awk '{ if (\$3 == "IN" && \$4 == "A") print \$5 }'")"
	fi

	echo $ip
}

# if non resolveable, returns empty string
cdepl_cluster_resolve_node_to_ip()
{
	local node=$1

	if [ "${__HHUBS_NODE_MAP[$node]}" = "" ]; then
		util_log_error_and_exit "[hhubs][$node] Resolve node to ip failed, remote node does not exist"
	fi

	cdepl_cluster_resolve_hostname_to_ip "${__HHUBS_NODE_MAP[$node]}"
}

cdepl_cluster_login_cmd()
{
	local cmd="$1"

	__cdepl_cluster_hhubs_remote_sollipulli_cmd "$cmd"
}

cdepl_cluster_send_file_to_login()
{
    local source="$1"
    local destination="$2"

    scp "$source" "${__HHUBS_SOLLIPULLI_USER}@sollipulli:${destination}"

	if [ "$__HHUBS_SOLLIPULLI_MODE" = "1" ]; then
		scp "$source" "${__HHUBS_SOLLIPULLI_USER}@sollipulli:${destination}"
	else
		scp -P 55517 "$source" "${__HHUBS_SOLLIPULLI_USER}@134.99.112.82:${destination}"
	fi
}

cdepl_cluster_node_cmd()
{
	local node=$1
	local cmd="$2"

	if [ "${__HHUBS_NODE_MAP[$node]}" = "" ]; then
		util_log_error_and_exit "[hhubs][node $node] Exec node $cmd, node does not exist"
	fi

	__cdepl_cluster_hhubs_remote_node_cmd ${__HHUBS_NODE_MAP[$node]} "$cmd"
}

cdepl_cluster_get_alloc_node_count()
{
	echo "$__HHUBS_TOTAL_NODES"
}

cdepl_cluster_node_resolve_node_to_hostname()
{
	local node=$1

	echo "${__HHUBS_NODE_MAP[$node]}"
}

####################################
# "private" callback methods that should not be called by the user

# "constructor"
_cdepl_cluster_on_init()
{
	local cluster_user=$1

	__HHUBS_SOLLIPULLI_USER="$cluster_user"

	__cdepl_cluster_hhubs_sollipulli_check
}

# Resolve alloc'd nodes to actual physical nodes on the cluster and fill the node map with
# available hostnames
_cdepl_cluster_on_node_setup_finish()
{
	local group1_req_cnt=0
	local group2_req_cnt=0
	local group3_req_cnt=0

	local group1_avail_cnt=0
	local group2_avail_cnt=0
	local group3_avail_cnt=0

	local group1_avail=""
	local group2_avail=""
	local group3_avail=""

	# Mode 0: nodes must be pre-allocated using the cluster node script
	# Mode 1: nodes are automatically allocated using the cluster node script
	local mode=0
	local node_cmd=""

	util_log_debug "[hhubs] on_node_setup_finish"

	if [ "$mode" = "0" ]; then
		# Get all currently allocated nodes
		node_cmd="__cdepl_cluster_hhubs_remote_sollipulli_cmd \"node ls-alloc\""
	else
		# Get all free nodes and alloc what we need later
		node_cmd="__cdepl_cluster_hhubs_remote_sollipulli_cmd \"node ls-free\""
	fi

	# Sort by groups
	for node in `eval $node_cmd`; do
		if [ "$(echo $__HHUBS_NODE_GROUP_1 | grep $node)" ]; then
			group1_avail="$group1_avail $node"
			group1_avail_cnt=$(($group1_avail_cnt + 1))
		elif [ "$(echo $__HHUBS_NODE_GROUP_2 | grep $node)" ]; then
			group2_avail="$group2_avail $node"
			group2_avail_cnt=$(($group2_avail_cnt + 1))
		elif [ "$(echo $__HHUBS_NODE_GROUP_3 | grep $node)" ]; then
			group3_avail="$group3_avail $node"
			group3_avail_cnt=$(($group3_avail_cnt + 1))
		fi
	done

	util_log_debug "[hhubs] Available instances: g1($group1_avail_cnt), g2($group2_avail_cnt), g3($group3_avail_cnt)"

	local insufficient_cnt=0

	# Assign instances with IB first
	for i in `seq 0 $((__HHUBS_TOTAL_NODES - 1))`; do
		if [ "${__HHUBS_NODES_NET[$i]}" = "ib" ]; then
			if [ "${__HHUBS_NODES_CPUS[$i]}" -le "4" ] && 
					[ "${__HHUBS_NODES_MEM[$i]}" -le "32768" ] && 
					[ "$group1_req_cnt" -lt "$group1_avail_cnt" ]; then
				group1_req_cnt=$(($group1_req_cnt + 1))
				
				# Mark what we want to alloc
				__HHUBS_NODE_MAP[$i]="group1"
			elif [ "$group3_req_cnt" -lt "$group3_avail_cnt" ]; then
				group3_req_cnt=$(($group3_req_cnt + 1))
				
				# Mark what we want to alloc
				__HHUBS_NODE_MAP[$i]="group3"
			else
				insufficient_cnt=$((insufficient_cnt + 1))
			fi
		fi
	done

	# Next iteration with eth, only
	for i in `seq 0 $((__HHUBS_TOTAL_NODES - 1))`; do
		if [ "${__HHUBS_NODES_NET[$i]}" != "ib" ]; then	
			if [ "${__HHUBS_NODES_CPUS[$i]}" -le "4" ] && 
					[ "${__HHUBS_NODES_MEM[$i]}" -le "16384" ] && 
					[ "$group2_req_cnt" -lt "$group2_avail_cnt" ]; then
				group2_req_cnt=$(($group2_req_cnt + 1))

				# Mark what we want to alloc
				__HHUBS_NODE_MAP[$i]="group2"
			elif [ "${__HHUBS_NODES_CPUS[$i]}" -le "4" ] && 
					[ "${__HHUBS_NODES_MEM[$i]}" -le "32768" ] && 
					[ "$group1_req_cnt" -lt "$group1_avail_cnt" ]; then
				group1_req_cnt=$(($group1_req_cnt + 1))

				# Mark what we want to alloc
				__HHUBS_NODE_MAP[$i]="group1"
			elif [ "$group3_req_cnt" -lt "$group3_avail_cnt" ]; then
				group3_req_cnt=$(($group3_req_cnt + 1))

				# Mark what we want to alloc
				__HHUBS_NODE_MAP[$i]="group3"
			else
				insufficient_cnt=$((insufficient_cnt + 1))
			fi
		fi
	done

	util_log_debug "[hhubs] Requested instances: g1($group1_req_cnt), g2($group2_req_cnt), g3($group3_req_cnt)"

	# Check if requested counts are available
	if [ "$insufficient_cnt" -gt "0" ]; then
		util_log_error_and_exit "[hhubs] Not enough instances of available: Lacking $insufficient_cnt instances"
	fi

	if [ "$group1_req_cnt" -gt "$group1_avail_cnt" ]; then
		util_log_error_and_exit "[hhubs] Not enough instances of group 1 available: requested $group1_req_cnt, available $group1_avail_cnt"
	fi

	if [ "$group2_req_cnt" -gt "$group2_avail_cnt" ]; then
		util_log_error_and_exit "[hhubs] Not enough instances of group 2 available: requested $group2_req_cnt, available $group2_avail_cnt"
	fi

	if [ "$group3_req_cnt" -gt "$group3_avail_cnt" ]; then
		util_log_error_and_exit "[hhubs] Not enough instances of group 3 available: requested $group3_req_cnt, available $group3_avail_cnt"
	fi

	local nodes=""

	# Generate node mappings using what's available
	# Also fill in hardware specs that are still default values with max
	# available
	for i in `seq 0 $((__HHUBS_TOTAL_NODES - 1))`; do
		nodes="${nodes}${i} ${__HHUBS_NODE_MAP[$i]}\n"

		# Replace marked groups to alloc with actual node hostnames
		case "${__HHUBS_NODE_MAP[$i]}" in
			"group1")
				__HHUBS_NODE_MAP[$i]=$(echo $group1_avail | cut -d " " -f1)
				group1_avail=$(echo $group1_avail | cut -d " " -f2-)

				if [ "${__HHUBS_NODES_CPUS[$i]}" = "0" ]; then
					__HHUBS_NODES_CPUS[$i]="$__HHUBS_NODE_GROUP_1_MAX_CORES"
				fi

				if [ "${__HHUBS_NODES_MEM[$i]}" = "0" ]; then
					__HHUBS_NODES_MEM[$i]="$__HHUBS_NODE_GROUP_1_MAX_RAM"
				fi

				if [ "${__HHUBS_NODES_NET[$i]}" = "" ]; then
					__HHUBS_NODES_NET[$i]="$__HHUBS_NODE_GROUP_1_MAX_NET"
				fi

				;;

			"group2")
				__HHUBS_NODE_MAP[$i]=$(echo $group2_avail | cut -d " " -f1)
				group2_avail=$(echo $group2_avail | cut -d " " -f2-)

				if [ "${__HHUBS_NODES_CPUS[$i]}" = "0" ]; then
					__HHUBS_NODES_CPUS[$i]="$__HHUBS_NODE_GROUP_2_MAX_CORES"
				fi

				if [ "${__HHUBS_NODES_MEM[$i]}" = "0" ]; then
					__HHUBS_NODES_MEM[$i]="$__HHUBS_NODE_GROUP_2_MAX_RAM"
				fi

				if [ "${__HHUBS_NODES_NET[$i]}" = "" ]; then
					__HHUBS_NODES_NET[$i]="$__HHUBS_NODE_GROUP_2_MAX_NET"
				fi

				;;

			"group3")
				__HHUBS_NODE_MAP[$i]=$(echo $group3_avail | cut -d " " -f1)
				group3_avail=$(echo $group3_avail | cut -d " " -f2-)

				if [ "${__HHUBS_NODES_CPUS[$i]}" = "0" ]; then
					__HHUBS_NODES_CPUS[$i]="$__HHUBS_NODE_GROUP_3_MAX_CORES"
				fi

				if [ "${__HHUBS_NODES_MEM[$i]}" = "0" ]; then
					__HHUBS_NODES_MEM[$i]="$__HHUBS_NODE_GROUP_3_MAX_RAM"
				fi

				if [ "${__HHUBS_NODES_NET[$i]}" = "" ]; then
					__HHUBS_NODES_NET[$i]="$__HHUBS_NODE_GROUP_3_MAX_NET"
				fi

				;;

			*)
				;;
		esac
	done

	util_log_debug "[hhubs] Node allocations: $nodes"

	# Store node mappings to file
	__cdepl_cluster_hhubs_remote_sollipulli_cmd "sync"
	__cdepl_cluster_hhubs_remote_sollipulli_cmd "printf \"$nodes\" > ${DEPLOY_CUR_OUT_PATH}/node_mappings && sync"
}

_cdepl_cluster_on_env_setup_finish()
{
	util_log_debug "[hhubs] on_env_setup_finish"
}

_cdepl_cluster_before_deploy()
{
	util_log_debug "[hhubs] before_deploy"
}

_cdepl_cluster_after_deploy()
{
	util_log_debug "[hhubs] after_deploy"
}

_cdepl_cluster_before_cleanup()
{
	util_log_debug "[hhubs] before_cleanup"
}

_cdepl_cluster_after_cleanup()
{
	util_log_debug "[hhubs] after_cleanup"
}

##################################
# private to this script

__cdepl_cluster_hhubs_remote_node_cmd()
{
	local node=$1
	local cmd="$2"

	local dest=""
	
	if [ "$__HHUBS_SOLLIPULLI_MODE" = "1" ]; then
		dest="${__HHUBS_SOLLIPULLI_USER}@sollipulli"
	else
		dest="${__HHUBS_SOLLIPULLI_USER}@134.99.112.82 -p 55517"
	fi

	# -n -f for nohup
	# Use ControlMaster to establish the TCP connection, once and keep it active
	# until all sessions are closed and no further session was established
	# within 60 seconds 
	# Note when using nohup: You must redirect stdin/stdout and not add any
	# command before it using &&, e.g. cd ~ && nohup ...
	# This will hang the session with controlmaster
	ssh -o ControlMaster=auto -o ControlPath=~/.ssh/cdepl_%r@%h:%p.sock -o ControlPersist=60 $dest -n -f "ssh $node \"$cmd\""
}

__cdepl_cluster_hhubs_remote_sollipulli_cmd()
{
	local cmd="$1"

	local dest=""
	
	if [ "$__HHUBS_SOLLIPULLI_MODE" = "1" ]; then
		dest="${__HHUBS_SOLLIPULLI_USER}@sollipulli"
	else
		dest="${__HHUBS_SOLLIPULLI_USER}@134.99.112.82 -p 55517"
	fi

	# -n -f for nohup
	# Use ControlMaster to establish the TCP connection, once and keep it active
	# until all sessions are closed and no further session was established
	# within 60 seconds 
	# Note when using nohup: You must redirect stdin/stdout and not add any
	# command before it using &&, e.g. cd ~ && nohup ...
	# This will hang the session with controlmaster
	ssh -o ControlMaster=auto -o ControlPath=~/.ssh/cdepl_%r@%h:%p.sock -o ControlPersist=60 $dest -n -f "$cmd"
}

__cdepl_cluster_hhubs_sollipulli_check()
{
	ping -c 1 sollipulli > /dev/null 2>&1

	# Check if sollipulli is reachable on the current net
	if [ "$?" != "0" ]; then

		# Try directly via IP
		ping -c 1 134.99.112.82 > /dev/null 2>&1

		if [ "$?" != "0" ]; then
			util_log_error_and_exit "[hhubs] Could not contact sollipulli, not available"
		else
			__HHUBS_SOLLIPULLI_MODE=2
		fi
	else
		__HHUBS_SOLLIPULLI_MODE=1
	fi

	# Check if passwordless auth available
	if [ "$__HHUBS_SOLLIPULLI_MODE" = "1" ]; then
		ssh -o PasswordAuthentication=no -o BatchMode=yes sollipulli exit &> /dev/null
	else
		ssh -p 55517 -o PasswordAuthentication=no -o BatchMode=yes ${__HHUBS_SOLLIPULLI_USER}@134.99.112.82 exit &> /dev/null
	fi

	if [ "$?" != "0" ]; then
		util_log_error_and_exit "[hhubs] Can't connect to login node, ensure passwordless auth is set up and the cluster user is valid"
	fi

	util_log_debug "[hhubs] sollipulli available, mode $__HHUBS_SOLLIPULLI_MODE"
}

__cdepl_cluster_hhubs_alloc_hardware_specs()
{
	TODO=""
}
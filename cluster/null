#!/bin/bash

#
# This module is not an implementation of a cluster system but provides all
# functions that must be implemented for the cluster abstraction layer.
# Some functions or feature might not be available or necessary on your
# cluster setup.
#
# You should checkout the existing implementations as well before starting your
# own. Either there is already one that supports your setup or you can benefit
# from it by copying great parts and tweaking it to match your setup.
#

#################################################
# Public API for deploy scripts
#################################################

##
# Allocate the specified number of nodes
#
# Either keep track of the number of nodes allocated, actually execute
# allocation here to physically claim the nodes or prepare to create a job
# script if your cluster runs a job system.
#
# $1: Number of nodes to allocate
##
cdepl_cluster_node_alloc()
{
	local num_nodes=$1

	# stub
}

##
# Get the number of allocated nodes
#
# ret stdout: Number of allocated nodes
##
cdepl_cluster_get_alloc_node_count()
{
	# stub
}

##
# Set the walltime when allocating resources
#
# If you are deploying against a job system, it might require you to specify
# the walltime of your job you want to run.
#
# $1: Walltime, format: HH:MM:SS
##
cdepl_cluster_walltime()
{
	local walltime=$1

	# stub
}

##
# Allocate all hardware resources available on a particular node
#
# If multiple jobs can be executed on the same node by the job system, you
# can specify exclusivity here. Note: Not necessarily supported or required
# to be implemented for every cluster
#
# $1: Node id of the node to allocate
##
cdepl_cluster_node_excl()
{
	local node=$1

	# stub
}

##
# Allocate a specific number of CPUs/cores on the target node
#
# If this feature can be implemented with your cluster, ensure that you check
# with the amount of available resources (if possible)
#
# $1: Node id of the node to allocate the resource on
# $2: Number of CPUs/cores to allocate
##
cdepl_cluster_node_cpus()
{
	local node=$1
	local cpus=$2

	# stub
}

##
# Allocate a specific amount of RAM on the target node
#
# If this feature can be implemented with your cluster, ensure that you check
# with the amount of available resources (if possible)
#
# $1: Node id of the node to allocate the resource on
# $2: Amount of memory (in MB) to allocate
##
cdepl_cluster_node_mem()
{
	local node=$1
	local mem=$2

	# stub
}

##
# Allocate a specific network type on the target node
#
# If this feature can be implemented with your cluster, ensure that you check
# if the target network hardware is available
#
# $1: Node id of the node to allocate the resource on
# $2: Network type to allocate: eth, ib
##
cdepl_cluster_node_network()
{
	local node=$1
	local net=$2

	# stub
}

##
# Resolve a hostname of a cluster node to the corresponding IPv4 address
#
# $1: hostname to resolve
# ret stdout: IPv4 address or empty string in non resolvable
##
cdepl_cluster_resolve_hostname_to_ip()
{
	local hostname=$1

	# stub
}

##
# Resolve an abstract node id of a cluster node to the corresponding IPv4 address
#
# $1: node id to resolve
# ret stdout: IPv4 address or empty string in non resolvable
##
cdepl_cluster_resolve_node_to_ip()
{
	local node=$1

	# stub
}

##
# Resolve an abstract node id of a cluster node to the hostname of the node
#
# $1: node id to resolve
# ret stdout: hostname of the node or empty string in non resolvable
##
cdepl_cluster_node_resolve_node_to_hostname()
{
	local node=$1

	# stub
}

##
# Send a command to a cluster node
#
# $1: node id of the node to send the command to
# $2: (ssh) command to send
# $3: An optional space separated list of required environments/dependencies
#     for this command, e.g. java/1.8 gcc/6.1
##
cdepl_cluster_node_cmd()
{
	local node=$1
	local cmd="$2"
	local required_env="$3"

	# stub
}

##
# Get all the log files from the nodes on a setup which doesn't use NFS.
#
# $1 Target destination to copy them to (local on the machine you run cdepl 
#    from)
# $2 The output path for log files, configs etc of the deployment on the nodes
##
cdepl_cluster_gather_log_files()
{
	local path="$1"
	local cur_path="$2"

	# stub
}

##
# Send a command for setting up files to all cluster nodes
#
# $1: (ssh) command to send
# $2: An optional space separated list of required environments/dependencies
#     for this command, e.g. java/1.8 gcc/6.1
##
cdepl_cluster_file_system_cmd()
{
	local cmd="$1"
	local required_env="$2"

	# stub
}

##
# Tell the caller if the current cluster allows executing commands using sudo
#
# $ret Empty string if not allowed and non empty string if allowed (e.g. "1")
##
cdepl_cluster_allows_sudo()
{
	echo ""
}

#################################################
# Callbacks for cdepl
#################################################

##
# "Constructor" of the cluster module
#
# Called right after the module is loaded into the cdepl environment
#
# $1: Name of the user to use when logging into the cluster
# ...: Further optional arguments that can be passed to cdepl_cluster_init
##
_cdepl_cluster_on_init()
{
	local cluster_user=$1

	util_log_debug "[null] on_init"
}

##
# Callback on node setup and allocation phase finish
##
_cdepl_cluster_on_node_setup_finish()
{
	util_log_debug "[null] on_node_setup_finish"
}

##
# Callback before deployment starts
##
_cdepl_cluster_before_deploy()
{
	util_log_debug "[null] before_deploy"
}

##
# Callback right after deployment finished
##
_cdepl_cluster_after_deploy()
{
	util_log_debug "[null] after_deploy"
}

##
# Callback right before cleanup
##
_cdepl_cluster_before_cleanup()
{
	util_log_debug "[null] before_cleanup"
}

##
# Callback right after cleanup
##
_cdepl_cluster_after_cleanup()
{
	util_log_debug "[null] after_cleanup"
}

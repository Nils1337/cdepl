#!/bin/bash

readonly __DXRAM_LOG_FILE_SUPERPEER_POSTFIX="_superpeer"
readonly __DXRAM_LOG_FILE_PEER_POSTFIX="_peer"

readonly __DXRAM_MAIN_CLASS="de.hhu.bsinfo.dxram.DXRAM"
readonly __DXRAM_CLASS_PATH="lib/slf4j-api-1.6.1.jar:lib/zookeeper-3.4.3.jar:lib/gson-2.7.jar:lib/log4j-api-2.7.jar:lib/log4j-core-2.7.jar:lib:dxram.jar"

readonly __DXRAM_DEFAULT_S_PORT="22221"
readonly __DXRAM_DEFAULT_P_PORT="22222"
readonly __DXRAM_DEFAULT_NETWORK="eth"
readonly __DXRAM_DEFAULT_MSG_HANDLER="2"
readonly __DXRAM_DEFAULT_P_KVSS="1024"

readonly __DXRAM_PROCESS_IDENTIFIER="dxramdeployscript"
readonly __DXRAM_DEFAULT_STARTUP_CONDITION="!---ooo---!"

readonly __DXRAM_REQUIRED_ENVIRONMENT="java/1.8 gcc/6.1"

__DXRAM_PATH=""
__DXRAM_CONFIG_PATH=""
__DXRAM_LOG4J_CONFIG_PATH=""

__DXRAM_OUT_PATH=""
__DXRAM_OUT_CONF_PATH=""
__DXRAM_OUT_LOG_PATH=""

__DXRAM_ZOOKEEPER_NODE=""
__DXRAM_ZOOKEEPER_PORT=""

__DXRAM_NODE_TYPE=()
__DXRAM_NODE_PORT=()
__DXRAM_NODE_NETWORK=()
__DXRAM_NODE_MSG_HANDLER=()
__DXRAM_NODE_RUN_SUDO=()
__DXRAM_NODE_REMOTE_DEBUG_PORT=()
__DXRAM_NODE_REMOTE_PROFILE_YJP_PORT=()
__DXRAM_NODE_REMOTE_PROFILE_YJP_AGENT_LIB=()
__DXRAM_NODE_PEER_KVSS=()

cdepl_app_dxram_init()
{
	local path=$1
	local zookeeper_node=$2
	local zookeeper_port=$3

	__DXRAM_PATH="$(cdepl_cluster_node_cmd 0 "readlink -f $path")"
	__DXRAM_CONFIG_PATH="${__DXRAM_PATH}/config/dxram.json"
	__DXRAM_LOG4J_CONFIG_PATH="${__DXRAM_PATH}/config/log4j.xml"

	__DXRAM_OUT_PATH="${__DEPLOY_CUR_OUT_PATH}/dxram"
	__DXRAM_OUT_CONF_PATH="${__DXRAM_OUT_PATH}/conf"
	__DXRAM_OUT_LOG_PATH="${__DXRAM_OUT_PATH}/log"

	__DXRAM_ZOOKEEPER_NODE="$zookeeper_node"
	__DXRAM_ZOOKEEPER_PORT="$zookeeper_port"

	# Check if dxram path is available
	if [ "$(cdepl_cluster_file_system_cmd "[ -d $__DXRAM_PATH ] && echo \"1\"")" != "1" ]; then
		util_log_error_and_exit "[login][dxram]: Path does not exist: $__DXRAM_PATH"
	fi

	__cdepl_app_dxram_check
	__cdepl_app_dxram_check_config

	# Output path setup
	cdepl_cluster_file_system_cmd "mkdir -p $__DXRAM_OUT_CONF_PATH"
	cdepl_cluster_file_system_cmd "mkdir -p $__DXRAM_OUT_LOG_PATH"

	util_log "[login][dxram] Initialized: $__DXRAM_PATH"
	util_log "[login][dxram] Output: $__DXRAM_OUT_PATH"
}

# in order to declare a node an actual DXRAM node, you have to assign a role
# no default role, node will be ignored and not considered a dxram node
cdepl_app_dxram_node_type()
{
	local node=$1
	local type=$2

	# No default values. If node type not set explicitly, node is not considered to run a DXRAM instance
	if [ "$node" -ge "$(cdepl_cluster_get_alloc_node_count)" ]; then
		util_log_error_and_exit "[$node][dxram] Invalid node id $node > $(cdepl_cluster_get_alloc_node_count)"
	fi

	if [ "$type" != "S" ] && [ "$type" != "P" ]; then
		util_log_error_and_exit "[$node][dxram] Invalid node type $type for node $node"
	fi

	__DXRAM_NODE_TYPE[$node]="$type"
}

cdepl_app_dxram_node_port()
{
	local node=$1
	local port=$2

	if [ "$node" -ge "$(cdepl_cluster_get_alloc_node_count)" ]; then
		util_log_error_and_exit "[$node][dxram] Invalid node id $node > $(cdepl_cluster_get_alloc_node_count)"
	fi

	if [ "$port" -gt "65536" ] || [ "$port" -lt "0" ]; then
		util_log_error_and_exit "[$node][dxram] Invalid port $port for node $node"
	fi

	__DXRAM_NODE_PORT[$node]="$port"
}

cdepl_app_dxram_node_network()
{
	local node=$1
	local network=$2

	if [ "$node" -ge "$(cdepl_cluster_get_alloc_node_count)" ]; then
		util_log_error_and_exit "[$node][dxram] Invalid node id $node > $(cdepl_cluster_get_alloc_node_count)"
	fi

	if [ "$network" != "eth" ] && [ "$network" != "ib" ]; then
		util_log_error_and_exit "[$node][dxram] Invalid network type $network for node $node"
	fi

	__DXRAM_NODE_NETWORK[$node]="$network"
}

cdepl_app_dxram_node_message_handler()
{
	local node=$1
	local msg_handler=$2

	if [ "$node" -ge "$(cdepl_cluster_get_alloc_node_count)" ]; then
		util_log_error_and_exit "[$node][dxram] Invalid node id $node > $(cdepl_cluster_get_alloc_node_count)"
	fi

	__DXRAM_NODE_MSG_HANDLER[$node]="$msg_handler"
}

cdepl_app_dxram_run_as_sudo()
{
	local node=$1

	if [ "$node" -ge "$(cdepl_cluster_get_alloc_node_count)" ]; then
		util_log_error_and_exit "[$node][dxram] Invalid node id $node > $(cdepl_cluster_get_alloc_node_count)"
	fi

	if [ "$(cdepl_cluster_allows_sudo)" ]; then
		__DXRAM_NODE_RUN_SUDO[$node]="1"
	else
		util_log_warn "[$node][dxram] Cluster type does not allow running commands as sudo, ignored"
	fi
}

cdepl_app_dxram_remote_debug()
{
	local node=$1
	local port=$2

	if [ "$node" -ge "$(cdepl_cluster_get_alloc_node_count)" ]; then
		util_log_error_and_exit "[$node][dxram] Invalid node id $node > $(cdepl_cluster_get_alloc_node_count)"
	fi

	__DXRAM_NODE_REMOTE_DEBUG_PORT[$node]="$port"
}

cdepl_app_dxram_remote_profile_yjp()
{
	local node=$1
	local port=$2
	local agent_lib=$3

	if [ "$node" -ge "$(cdepl_cluster_get_alloc_node_count)" ]; then
		util_log_error_and_exit "[$node][dxram] Invalid node id $node > $(cdepl_cluster_get_alloc_node_count)"
	fi

	# Check if the agent lib is available
	if [ "$(cdepl_cluster_node_cmd $node "[ -f $agent_lib ] && echo \"1\"")" != "1" ]; then
		util_log_error_and_exit "[$node][dxram] Could not find libyjpagent.so in $agent_lib"
	fi

	__DXRAM_NODE_REMOTE_PROFILE_YJP_PORT[$node]="$port"
	__DXRAM_NODE_REMOTE_PROFILE_YJP_AGENT_LIB[$node]="$agent_lib"
}

cdepl_app_dxram_peer_kvss()
{
	local node=$1
	local kvss=$2

	if [ "$node" -ge "$(cdepl_cluster_get_alloc_node_count)" ]; then
		util_log_error_and_exit "[$node][dxram] Invalid node id $node > $(cdepl_cluster_get_alloc_node_count)"
	fi

	__DXRAM_NODE_PEER_KVSS[$node]="$kvss"
}

cdepl_app_dxram_auto_detect_node_config_params()
{
	local node=$1

	# TODO: auto detect optimal
	# network: if /dev/infiniband -> ib, eth otherwise
	# msg handler count: total core count - 2
	# kvss: 1GB OS, 2GB jvm, remain: kvss

	util_log_error_and_exit "Auto detect node config and params not implemented, yet"
}

cdepl_app_dxram_start_node()
{
	local node=$1

	if [ "${__DXRAM_NODE_TYPE[$node]}" = "" ]; then
		util_log_error_and_exit "[$node][dxram] No node type set, cannot start instance"
	fi

	local logfile=""

	if [ "${__DXRAM_NODE_TYPE[$node]}" = "S" ]; then
		logfile=${__DXRAM_OUT_LOG_PATH}/node${node}${__DXRAM_LOG_FILE_SUPERPEER_POSTFIX}
	else
		logfile=${__DXRAM_OUT_LOG_PATH}/node${node}${__DXRAM_LOG_FILE_PEER_POSTFIX}
	fi

	__cdepl_app_dxram_resolve_default_config_values
	__cdepl_app_dxram_create_node_base_config $node ${__DXRAM_NODE_TYPE[$node]}

	if [ "${__DXRAM_NODE_TYPE[$node]}" = "S" ]; then
		__cdepl_app_dxram_start_superpeer $node $logfile
	elif [ "${__DXRAM_NODE_TYPE[$node]}" = "P" ]; then
		__cdepl_app_dxram_start_peer $node $logfile
	fi
}

# Compared to cleanup, this executes a soft shutdown calling a dxram node and initiating a clean shutdown
cdepl_app_dxram_shutdown_node()
{
	local node=$1
	local shutdown_type=$2

	# TODO
	util_log_error_and_exit "Soft shutdown not implemented"
}

# second parameter condition optional, using default start condition otherwise
cdepl_app_dxram_node_wait_started()
{
	local node=$1
	local condition=$2

	local type="${__DXRAM_NODE_TYPE[$node]}"
	local logfile=""

	if [ "${__DXRAM_NODE_TYPE[$node]}" = "S" ]; then
		logfile=${__DXRAM_OUT_LOG_PATH}/node${node}${__DXRAM_LOG_FILE_SUPERPEER_POSTFIX}
	else
		logfile=${__DXRAM_OUT_LOG_PATH}/node${node}${__DXRAM_LOG_FILE_PEER_POSTFIX}
	fi

	# Use default condition if not specfied
	if [ ! "$condition" ]; then
		condition="$__DXRAM_DEFAULT_STARTUP_CONDITION"
	fi

	util_log "[$node][dxram][$type] Waiting for startup: $condition"

	while true; do
		echo -n "."

		local success=$(cdepl_cluster_node_cmd $node "cat $logfile 2> /dev/null | sed 's,\x1B\[[0-9;]*[a-zA-Z],,g' | grep '$condition'")
		local fail_init=$(cdepl_cluster_node_cmd $node "cat $logfile 2> /dev/null | sed 's,\x1B\[[0-9;]*[a-zA-Z],,g' | grep '^Initializing DXRAM failed.$'")
		# Abort execution after an exception was thrown (every exception but NetworkResponseCancelledException)
		local fail_error=$(cdepl_cluster_node_cmd $node "cat $logfile 2> /dev/null | sed 's,\x1B\[[0-9;]*[a-zA-Z],,g' | grep -i 'exception' | grep -v 'NetworkResponseCancelledException'")
		# "A fatal error" -> JVM segfaults
		local fail_error2=$(cdepl_cluster_node_cmd $node "cat $logfile 2> /dev/null | sed 's,\x1B\[[0-9;]*[a-zA-Z],,g' | grep -i -e '\[ERROR\]' -e '\# A fatal error'")

		if [ "$success" ]; then
			local pid=$(__cdepl_app_dxram_get_instance_running_pid $node ${DXRAM_YCSB_NODE_PORT[$node]})

			echo ""

			if [ ! "$pid" ]; then
				util_log_error_and_exit "[$node][dxram][$type] Could not find started process"
			fi

			util_log "[$node][dxram][$type] Started (pid: $pid)"

			break
		elif [ "$fail_init" ]; then
			echo ""
			util_log_error_and_exit "[$node][dxram][$type] Could not be started. See log file $logfile"
			return 1
		elif [ "$fail_error" ] || [ "$fail_error2" ]; then
			echo ""
			util_log_error_and_exit "[$node][dxram][$type] Failed, error or exception. See log file $logfile"
			return 2
		fi

		sleep 1.0
	done

	return 0
}

cdepl_app_dxram_node_cleanup()
{
	local node=$1

	util_log "[$node][dxram] Cleanup..."

	__cdepl_app_dxram_cleanup $node
}

#################################################

__cdepl_app_dxram_check()
{
	if [ "$(cdepl_cluster_node_cmd 0 "[ -f ${__DXRAM_PATH}/dxram.jar ] && echo 1")" != "1" ]; then
		util_log_error_and_exit "[0][dxram] Could not find dxram.jar in $__DXRAM_PATH"
	fi
}

__cdepl_app_dxram_check_config()
{
	# Check if config file is available and create default config
	if [ "$(cdepl_cluster_node_cmd 0 "[ -f $__DXRAM_CONFIG_PATH ] && echo \"1\"")" != "1" ]; then
		util_log "[0][dxram] No config file available, creating default config: $__DXRAM_CONFIG_PATH"

		# Don't run this on the login node (might not have java installed)
		# Use the first actual cluster node instead
		cdepl_cluster_node_cmd 0 "cd $__DXRAM_PATH && java -Dlog4j.configurationFile=$__DXRAM_LOG4J_CONFIG_PATH -Ddxram.config=$__DXRAM_CONFIG_PATH -cp $__DXRAM_CLASS_PATH $__DXRAM_MAIN_CLASS > /dev/null 2>&1" "$__DXRAM_REQUIRED_ENVIRONMENT"

		# Sanity check
		if [ "$(cdepl_cluster_node_cmd 0 "[ -f $__DXRAM_CONFIG_PATH ] && echo \"1\"")" != "1" ]; then
			util_log_error_and_exit "[0][dxram] Creating config file $__DXRAM_CONFIG_PATH failed"
		fi
	else
		local config_content="$(cdepl_cluster_node_cmd 0 "cat "$__DXRAM_CONFIG_PATH"")"
		# Check if corrupted configuration file
		local component_header=`echo $config_content | grep "m_componentConfigs"`
		local service_header=`echo $config_content | grep "m_serviceConfigs"`
		if [ "$component_header" = "" ] && [ "$service_header" = "" ] ; then
			util_log "[0][dxram] Configuration file $__DXRAM_CONFIG_PATH corrupted, deleting and creating default"

			# Configuration file seems to be corrupted -> start dxram once to create new configuration
			cdepl_cluster_node_cmd 0 "rm $__DXRAM_CONFIG_PATH && cd $__DXRAM_PATH && java -Dlog4j.configurationFile=$__DXRAM_LOG4J_CONFIG_PATH -Ddxram.config=$__DXRAM_CONFIG_PATH -cp $__DXRAM_CLASS_PATH $__DXRAM_MAIN_CLASS && sync > /dev/null 2>&1" "$__DXRAM_REQUIRED_ENVIRONMENT"

			# Sanity check
			if [ "$(cdepl_cluster_node_cmd 0 "[ -f $__DXRAM_CONFIG_PATH ] && echo \"1\"")" != "1" ]; then
				util_log_error_and_exit "[0][dxram] Creating config file $__DXRAM_CONFIG_PATH failed"
			fi
		fi
	fi
}

__cdepl_app_dxram_resolve_default_config_values()
{
	local node_count="$(cdepl_cluster_get_alloc_node_count)"

	for i in `seq 0 $((node_count - 1))`; do
		# Only for explicitly set node types = DXRAM instance
		if [ "${__DXRAM_NODE_TYPE[$i]}" = "S" ] && [ "${__DXRAM_NODE_PORT[$i]}" = "" ]; then
			__DXRAM_NODE_PORT[$i]="$__DXRAM_DEFAULT_S_PORT"
		fi

		if [ "${__DXRAM_NODE_TYPE[$i]}" = "P" ] && [ "${__DXRAM_NODE_PORT[$i]}" = "" ]; then
			__DXRAM_NODE_PORT[$i]="$__DXRAM_DEFAULT_P_PORT"
		fi

		if [ "${__DXRAM_NODE_TYPE[$i]}" = "P" ] && [ "${__DXRAM_NODE_PEER_KVSS[$i]}" = "" ]; then
			__DXRAM_NODE_PEER_KVSS[$i]="$__DXRAM_DEFAULT_P_KVSS"
		fi

		if [ "${__DXRAM_NODE_TYPE[$i]}" != "" ]; then
			if [ "${__DXRAM_NODE_NETWORK[$i]}" = "" ]; then
				__DXRAM_NODE_NETWORK[$i]="$__DXRAM_DEFAULT_NETWORK"
			fi

			if [ "${__DXRAM_NODE_MSG_HANDLER[$i]}" = "" ]; then
				__DXRAM_NODE_MSG_HANDLER[$i]="$__DXRAM_DEFAULT_MSG_HANDLER"
			fi
		fi
	done
}

__cdepl_app_dxram_create_node_base_config()
{
	local node=$1
	local type=$2

	local node_config_path="${__DXRAM_OUT_CONF_PATH}/node_${node}.conf"

	local zookeeper_ip="$(cdepl_cluster_resolve_node_to_ip $__DXRAM_ZOOKEEPER_NODE)"
	local node_config="$(cdepl_cluster_node_cmd 0 "cat $__DXRAM_CONFIG_PATH")"

	if [ zookeeper_ip = "" ]; then
		util_log_error_and_exit "[$node][dxram] Could not resolve zookeeper ip for node $__DXRAM_ZOOKEEPER_NODE"
	fi

	# Insert zookeeper config values
	# Create replacement string for zookeeper configuration
	local zookeeper_config_string="
		\"m_path\": \"/dxram\",
		\"m_connection\": {
			\"m_ip\": \"$zookeeper_ip\",
			\"m_port\": $__DXRAM_ZOOKEEPER_PORT
		},"

	# Replace zookeeper configuration
	local current_node_config="$(echo "$node_config" | sed '/ZookeeperBootComponentConfig/q')"
	current_node_config="${current_node_config}${zookeeper_config_string}"
	local end="$(echo "$node_config" | sed -ne '/ZookeeperBootComponentConfig/{s///; :a' -e 'n;p;ba' -e '}')"
	end="$(echo "$end" | sed -ne '/},/{s///; :a' -e 'n;p;ba' -e '}')"
	current_node_config="$(echo -e "${current_node_config}\n${end}")"

	node_config="$current_node_config"

	# Insert node config mappings
	# Create replacement string for nodes configuration:
	local default_node="{
		\"m_address\": {
			\"m_ip\": \"IP_TEMPLATE\",
			\"m_port\": PORT_TEMPLATE
		},
		\"m_role\": \"ROLE_TEMPLATE\",
		\"m_rack\": 0,
		\"m_switch\": 0,
		\"m_readFromFile\": 1
	}"

	local node_config_string=""
	local first_iterartion=true

	# Create "List" of node configs for configuration file
	for i in `seq 0 $(($(cdepl_cluster_get_alloc_node_count) - 1))`; do
		local ip=""
		local port=""
		local role=""

		ip="$(cdepl_cluster_resolve_node_to_ip "$i")"

		if [ ! "ip" ]; then
			util_log_error_and_exit "[$node][dxram] Could not resolve node to ip"
		fi

		port="${__DXRAM_NODE_PORT[$i]}"

		# Only for explicitly set node types = DXRAM instance
		if [ "${__DXRAM_NODE_TYPE[$i]}" = "S" ]; then
			role="SUPERPEER"
		elif [ "${__DXRAM_NODE_TYPE[$i]}" = "P" ]; then
			role="PEER"
		fi

		if [ "${__DXRAM_NODE_TYPE[$i]}" ]; then
			local node_string=`echo "$default_node" | sed "s/IP_TEMPLATE/$ip/" | sed "s/PORT_TEMPLATE/$port/" | sed "s/ROLE_TEMPLATE/$role/"`

			# Separate items of list with ,
			if [ "$first_iterartion" == true ]; then
				node_config_string="${node_config_string}${node_string}"
				first_iterartion=false
			else
				node_config_string="${node_config_string},${node_string}"
			fi
		fi
	done

	# Close node config list
	node_config_string="$(echo -e "$node_config_string\n      ],")"

	# Replace nodes configuration:
	local new_config="$(echo "${node_config}" | sed '/m_nodesConfig/q')"
	new_config="${new_config}${node_config_string}"
	local end="$(echo "${node_config}" | sed -ne '/m_nodesConfig/{s///; :a' -e 'n;p;ba' -e '}')"
	end="$(echo "${end}" | sed -ne '/],/{s///; :a' -e 'n;p;ba' -e '}')"
	new_config="$(echo -e "${new_config}\n${end}")"

	# Write back new config
	cdepl_cluster_node_cmd $node "echo '$new_config' > '${node_config_path}'"
}

__cdepl_app_dxram_start_superpeer()
{
	local node=$1
	local logfile=$2

	local node_config_path="${__DXRAM_OUT_CONF_PATH}/node_${node}.conf"

	util_log "[$node][dxram][S] Starting superpeer, logfile: $logfile config: $node_config_path"

	local ip="$(cdepl_cluster_resolve_hostname_to_ip "$(cdepl_cluster_node_resolve_node_to_hostname $node)")"

	if [ ! "$ip" ]; then
		util_log_error_and_exit "[$node][dxram][S] Could not resolve hostname '$(cdepl_cluster_node_resolve_node_to_hostname $node)' to ip"
	fi

	local vm_opts=""

	# Required to fix JNI crashing with libIbdxnet (see JNINotes.md in ibnet repository)
	vm_opts="-XX:+UseMembar"

	vm_opts="$vm_opts -Dlog4j.configurationFile=$__DXRAM_LOG4J_CONFIG_PATH"
	vm_opts="$vm_opts -Ddxram.config=$node_config_path"
	vm_opts="$vm_opts -Ddxram.m_config.m_engineConfig.m_address.m_ip=$ip"
	vm_opts="$vm_opts -Ddxram.m_config.m_engineConfig.m_address.m_port=${__DXRAM_NODE_PORT[$node]}"
	vm_opts="$vm_opts -Ddxram.m_config.m_engineConfig.m_role=Superpeer"

	# Optional dxram node specific settings

	if [ "${__DXRAM_NODE_NETWORK[$node]}" = "eth" ]; then
		vm_opts="$vm_opts -Ddxram.m_config.m_componentConfigs[NetworkComponentConfig].m_core.m_device=Ethernet"
	elif [ "${__DXRAM_NODE_NETWORK[$node]}" = "ib" ]; then
		vm_opts="$vm_opts -Ddxram.m_config.m_componentConfigs[NetworkComponentConfig].m_core.m_device=Infiniband"
	fi

	if [ "${__DXRAM_NODE_MSG_HANDLER[$node]}" ]; then
		vm_opts="$vm_opts -Ddxram.m_config.m_componentConfigs[NetworkComponentConfig].m_core.m_numMessageHandlerThreads=${__DXRAM_NODE_MSG_HANDLER[$node]}"
	fi

	# Development and debugging

	local root=""
	if [ "${__DXRAM_NODE_RUN_SUDO[$node]}" = "1" ]; then
		util_log "[$node][dxram][S] Running with sudo"
		root="sudo -P"
	fi

	if [ "${__DXRAM_NODE_REMOTE_DEBUG_PORT[$node]}" != "" ]; then
		vm_opts="$vm_opts -agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=${__DXRAM_NODE_REMOTE_DEBUG_PORT[$node]}"
		util_log "[$node][dxram][S] Enabled remote debugging on port ${__DXRAM_NODE_REMOTE_DEBUG_PORT[$node]}"
		util_log "[$node][dxram][S] On your local machine: establish a tunnel using 'ssh <target_hostname> -L ${__DXRAM_NODE_REMOTE_DEBUG_PORT[$node]}:<target_hostname>:${__DXRAM_NODE_REMOTE_DEBUG_PORT[$node]}' and connect your debugger to localhost, port ${__DXRAM_NODE_REMOTE_DEBUG_PORT[$node]}"
	fi

	if [ "${__DXRAM_NODE_REMOTE_PROFILE_YJP_PORT[$node]}" != "" ]; then
		vm_opts="$vm_opts -agentpath:${__DXRAM_NODE_REMOTE_PROFILE_YJP_AGENT_LIB[$node]}=port=${__DXRAM_NODE_REMOTE_PROFILE_YJP_PORT[$node]}"

		util_log "[$node][dxram][S] On your local machine: establish a tunnel using 'ssh <target_hostname> -L ${__DXRAM_NODE_REMOTE_PROFILE_YJP_PORT[$node]}:<target_hostname>:${__DXRAM_NODE_REMOTE_PROFILE_YJP_PORT[$node]}' and connect with yourkit using 'Connect to remote application' with the arguments 'localhost:${__DXRAM_NODE_REMOTE_PROFILE_YJP_PORT[$node]}'"
	fi

	# Don't use && instead of ;
	# This will hang if the ssh with controlmaster and nohup is used
	cdepl_cluster_node_cmd $node "cd $__DXRAM_PATH ; $root nohup java -D${__DXRAM_PROCESS_IDENTIFIER} $vm_opts -cp $__DXRAM_CLASS_PATH $__DXRAM_MAIN_CLASS > $logfile 2>&1 &" "$__DXRAM_REQUIRED_ENVIRONMENT"
}

__cdepl_app_dxram_start_peer()
{
	local node=$1
	local logfile=$2

	local node_config_path="${__DXRAM_OUT_CONF_PATH}/node_${node}.conf"

	util_log "[$node][dxram][P] Starting peer, logfile: $logfile config: $node_config_path"

	local ip="$(cdepl_cluster_resolve_hostname_to_ip "$(cdepl_cluster_node_resolve_node_to_hostname $node)")"

	if [ ! "$ip" ]; then
		util_log_error_and_exit "[$node][dxram][P] Could not resolve hostname '$(cdepl_cluster_node_resolve_node_to_hostname $node)' to ip"
	fi

	local vm_opts=""

	# Required to fix JNI crashing with libIbdxnet (see JNINotes.md in ibnet repository)
	vm_opts="-XX:+UseMembar"

	vm_opts="$vm_opts -Dlog4j.configurationFile=$__DXRAM_LOG4J_CONFIG_PATH"
	vm_opts="$vm_opts -Ddxram.config=$node_config_path"
	vm_opts="$vm_opts -Ddxram.m_config.m_engineConfig.m_address.m_ip=$ip"
	vm_opts="$vm_opts -Ddxram.m_config.m_engineConfig.m_address.m_port=${__DXRAM_NODE_PORT[$node]}"
	vm_opts="$vm_opts -Ddxram.m_config.m_engineConfig.m_role=Peer"

	# Optional dxram node specific settings

	if [ "${__DXRAM_NODE_NETWORK[$node]}" = "eth" ]; then
		vm_opts="$vm_opts -Ddxram.m_config.m_componentConfigs[NetworkComponentConfig].m_core.m_device=Ethernet"
	elif [ "${__DXRAM_NODE_NETWORK[$node]}" = "ib" ]; then
		vm_opts="$vm_opts -Ddxram.m_config.m_componentConfigs[NetworkComponentConfig].m_core.m_device=Infiniband"
	fi

	if [ "${__DXRAM_NODE_MSG_HANDLER[$node]}" ]; then
		vm_opts="$vm_opts -Ddxram.m_config.m_componentConfigs[NetworkComponentConfig].m_core.m_numMessageHandlerThreads=${__DXRAM_NODE_MSG_HANDLER[$node]}"
	fi

	if [ "${__DXRAM_NODE_PEER_KVSS[$node]}" ]; then
		vm_opts="$vm_opts -Ddxram.m_config.m_componentConfigs[MemoryManagerComponentConfig].m_keyValueStoreSize.m_value=${__DXRAM_NODE_PEER_KVSS[$node]}"
		vm_opts="$vm_opts -Ddxram.m_config.m_componentConfigs[MemoryManagerComponentConfig].m_keyValueStoreSize.m_unit=mb"
	fi

	# Development and debugging

	local root=""
	if [ "${__DXRAM_NODE_RUN_SUDO[$node]}" = "1" ]; then
		util_log "[$node][dxram][P] Running with sudo"
		root="sudo -P"
	fi

	if [ "${__DXRAM_NODE_REMOTE_DEBUG_PORT[$node]}" != "" ]; then
		vm_opts="$vm_opts -agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=${__DXRAM_NODE_REMOTE_DEBUG_PORT[$node]}"
		util_log "[$node][dxram][P] Enabled remote debugging on port ${__DXRAM_NODE_REMOTE_DEBUG_PORT[$node]}"
		util_log "[$node][dxram][P] On your local machine: establish a tunnel using 'ssh <target_hostname> -L ${__DXRAM_NODE_REMOTE_DEBUG_PORT[$node]}:<target_hostname>:${__DXRAM_NODE_REMOTE_DEBUG_PORT[$node]}' and connect your debugger to localhost, port ${__DXRAM_NODE_REMOTE_DEBUG_PORT[$node]}"
	fi

	if [ "${__DXRAM_NODE_REMOTE_PROFILE_YJP_PORT[$node]}" != "" ]; then
		vm_opts="$vm_opts -agentpath:${__DXRAM_NODE_REMOTE_PROFILE_YJP_AGENT_LIB[$node]}=port=${__DXRAM_NODE_REMOTE_PROFILE_YJP_PORT[$node]}"

		util_log "[$node][dxram][P] On your local machine: establish a tunnel using 'ssh <target_hostname> -L ${__DXRAM_NODE_REMOTE_PROFILE_YJP_PORT[$node]}:<target_hostname>:${__DXRAM_NODE_REMOTE_PROFILE_YJP_PORT[$node]}' and connect with yourkit using 'Connect to remote application' with the arguments 'localhost:${__DXRAM_NODE_REMOTE_PROFILE_YJP_PORT[$node]}'"
	fi

	# Don't use && instead of ;
	# This will hang if the ssh with controlmaster and nohup is used
	cdepl_cluster_node_cmd $node "cd $__DXRAM_PATH ; $root nohup java -D${__DXRAM_PROCESS_IDENTIFIER} $vm_opts -cp $__DXRAM_CLASS_PATH $__DXRAM_MAIN_CLASS > $logfile 2>&1 &" "req_env"
}

__cdepl_app_dxram_get_instance_running_pid()
{
	local node=$1
	local port=$2

	if [ "$port" ]; then
		port=".*-Ddxram.m_config.m_engineConfig.m_address.m_port=${port}"
	fi

	# Consider port for multiple instances on a single machine (e.g. localhost)
	echo "$(cdepl_cluster_node_cmd $node "pgrep -f '^java.*${__DXRAM_PROCESS_IDENTIFIER}${port}'")"
}

__cdepl_app_dxram_cleanup()
{
	local node=$1

	local pid=$(__cdepl_app_dxram_get_instance_running_pid $node)

	if [ "$pid" ]; then
		# If we or someone else left some garbage processes on the node multiple
		# pids are returned
		for i in $pid; do
			local kill_out=$(cdepl_cluster_node_cmd $node "kill -9 $i 2>&1")

			if [ "$?" = "0" ] && [ ! "$kill_out" ]; then
				util_log "[$node][dxram] Killed (pid: $i)"
			elif [ "$kill_out" ]; then
				# Probably operation not permitted, try root
				cdepl_cluster_node_cmd $node "sudo -P kill -9 $i > /dev/null 2>&1"

				if [ "$?" = "0" ]; then
					util_log "[$node][dxram] Killed (root) (pid: $i)"
				elif [ "$?" != "1" ]; then
					util_log_warn "[$node][dxram] Killing (root) $i failed, DXRAM instance(s) might stay alive"
				fi
			elif [ "$?" != "1" ]; then
				util_log_warn "[$node][dxram] Killing $i failed, DXRAM instance(s) might stay alive"
			fi
		done
	fi
}

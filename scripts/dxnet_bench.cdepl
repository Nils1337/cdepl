#!/bin/bash

#
# Comprehensive benchmark for DXNet
#

# Adjust parameters according to your setup
NETWORK_TYPE="ib"

# Total nodes to use
TOTAL_NODES=""

# Cluster type and user selected
CLUSTER_TYPE=""
CLUSTER_USER=""
CLUSTER_TYPE_INIT_ARGS=""

# Output path for log files, config files depending on the application deployed
OUT_PATH=""

# DXNet path
DXNET_PATH=""

# All results are compressed and stored here
RESULT_ARCHIVES_OUT_PATH=""

# TODO for latency measure up to X bytes (see paper)                                      
# TODO for throughput start at X bytes (see paper) and up to 1MB                          
# TODO for throughput measurements (all of them): calculate total throughput              
# and adjust message count accordingly, more messages for smaller sizes                   
# and once saturation (should) kick in: cap with specific total amount 

##
# Run a one-to-one benchmark. One node sends data to another node
# (not full duplex)
#
# $1 Workload id (0, 1, 2, 3)
# $2 Number of messages per node to send to other nodes (in total)
# $3 Payload size for the messages to send
# $4 Number of application/send threads
# $5 Number of message handlers for receiving incoming messages
# $6 Name of the benchmark configuration (for grouping the archives of the
#    benchmark results)
##
benchmark_one_to_one()
{
	local workload=$1
	local msg_per_node=$2
	local msg_size=$3
	local threads=$4
	local msg_handler=$5
	local benchmark_name=$6

	echo "##### One-to-one benchmark: $workload $msg_per_node $msg_size $threads $msg_handler"

	# Initialize dxnet deployment
	cdepl_app_dxnet_init $DXNET_PATH

	# Kill any still running instances from previous deployments
	cdepl_app_dxnet_node_cleanup 0 $((TOTAL_NODES - 1))

	# One to one
	# One node sends (only) another node receives (only)
	cdepl_app_dxnet_node_send_targets 0 1

	# Sender
	depl_app_dxnet_msg_send_count 0 $msg_per_node
	depl_app_dxnet_msg_recv_count 0 0
	cdepl_app_dxnet_node_send_threads 0 $threads
	cdepl_app_dxnet_node_message_handler 0 1

	# Receiver
	depl_app_dxnet_msg_send_count 1 0
	depl_app_dxnet_msg_recv_count 1 $msg_per_node
	cdepl_app_dxnet_node_send_threads 1 0
	cdepl_app_dxnet_node_message_handler 1 $msg_handler

	# Set network type
	depl_app_dxnet_network $NETWORK_TYPE

	# Set further parameters for nodes
	for i in $(seq 0 1); do
		depl_app_dxnet_workload $i $workload
		depl_app_dxnet_msg_size $i $msg_size

		if [ "$NETWORK_TYPE" = "ib" ]; then
			cdepl_app_dxnet_run_as_sudo $i
		fi
	done

	# Start all instances
	cdepl_app_dxnet_start_node 0 1

	# Wait for all instances to finish, this also checks for runtime errors
	for i in $(seq 0 1); do
		cdepl_app_dxnet_node_wait_finished $i
	done

	# Print results
	for i in $(seq 0 1); do
		printf "######################\nResults node $i\n"
		cdepl_app_dxnet_node_get_results $i
		printf "######################\n"
	done

	# Kill any leftovers
	cdepl_app_dxnet_node_cleanup 0 1

	# Pack results and move
	cdepl_deploy_archive_out_path "dxnet_${workload}_${total_nodes}_${msg_size}_${threads}_${msg_handler}" ${RESULT_ARCHIVES_OUT_PATH}/${benchmark_name}

	# Reset output path for next benchmark call
	cdepl_deploy_out_path $OUT_PATH
}

##
# Run an all-to-all benchmark. Use this to run point-to-point benchmarks by
# setting the number of nodes to 2.
#
# $1 Workload id (0, 1, 2, 3)
# $2 Total number of nodes to use for the benchmark
# $3 Number of messages per node to send to other nodes (in total)
# $4 Payload size for the messages to send
# $5 Number of application/send threads
# $6 Number of message handlers for receiving incoming messages
# $7 Name of the benchmark configuration (for grouping the archives of the
#    benchmark results)
##
benchmark_all_to_all()
{
	local workload=$1
	local total_nodes=$2
	local msg_per_node=$3
	local msg_size=$4
	local threads=$5
	local msg_handler=$6
	local benchmark_name=$7

	echo "##### All-to-all benchmark: $workload $total_nodes $msg_per_node $msg_size $threads $msg_handler"

	if [ "$total_nodes" -lt "2" ]; then
		util_log_error_and_exit "Benchmark needs at least two nodes, provided: $total_nodes"
	fi

	# Initialize dxnet deployment
	cdepl_app_dxnet_init $DXNET_PATH

	# Kill any still running instances from previous deployments
	cdepl_app_dxnet_node_cleanup 0 $((total_nodes - 1))

	# All to all
	# On two nodes only -> point to point
	for i in $(seq 0 $((total_nodes - 1))); do
		local targets=""

		for j in $(seq 0 $((total_nodes - 1))); do
			if [ "$i" != "$j" ]; then
				targets="$targets $j"
			fi
		done

		cdepl_app_dxnet_node_send_targets $i $targets
	done

	# Set network type
	depl_app_dxnet_network $NETWORK_TYPE

	# Set further parameters for nodes
	for i in $(seq 0 $((total_nodes - 1))); do
		depl_app_dxnet_workload $i $workload
		depl_app_dxnet_msg_send_count $i $msg_per_node
		depl_app_dxnet_msg_recv_count $i $((msg_per_node * (total_nodes - 1)))
		depl_app_dxnet_msg_size $i $msg_size

		cdepl_app_dxnet_node_send_threads $i $threads
		cdepl_app_dxnet_node_message_handler $i $msg_handler

		if [ "$NETWORK_TYPE" = "ib" ]; then
			cdepl_app_dxnet_run_as_sudo $i
		fi
	done

	# Start all instances
	cdepl_app_dxnet_start_node 0 $((total_nodes - 1))

	# Wait for all instances to finish, this also checks for runtime errors
	for i in $(seq 0 $((total_nodes - 1))); do
		cdepl_app_dxnet_node_wait_finished $i
	done

	# Print results
	for i in $(seq 0 $((total_nodes - 1))); do
		printf "######################\nResults node $i\n"
		cdepl_app_dxnet_node_get_results $i
		printf "######################\n"
	done

	# Kill any leftovers
	cdepl_app_dxnet_node_cleanup 0 $((total_nodes - 1))

	# Pack results and move
	cdepl_deploy_archive_out_path "dxnet_${workload}_${total_nodes}_${msg_size}_${threads}_${msg_handler}" ${RESULT_ARCHIVES_OUT_PATH}/${benchmark_name}

	# Reset output path for next benchmark call
	cdepl_deploy_out_path $OUT_PATH
}

##
# Run a benchmark which creates a ring structure with the specified number of
# nodes. One node will always send to its predecessor and receive from its
# successor.
#
# $1 Workload id (0, 1, 2, 3)
# $2 Total number of nodes to use for the benchmark
# $3 Number of messages per node to send to its successor
# $4 Payload size for the messages to send
# $5 Number of application/send threads
# $6 Number of message handlers for receiving incoming messages
# $7 Name of the benchmark configuration (for grouping the archives of the
#    benchmark results)
##
benchmark_ring()
{
	local workload=$1
	local total_nodes=$2
	local msg_per_node=$3
	local msg_size=$4
	local threads=$5
	local msg_handler=$6
	local benchmark_name=$7

	echo "##### Ring benchmark: $workload $total_nodes $msg_per_node $msg_size $threads $msg_handler"

	if [ "$total_nodes" -lt "2" ]; then
		util_log_error_and_exit "Benchmark needs at least two nodes, provided: $total_nodes"
	fi

	# Initialize dxnet deployment
	cdepl_app_dxnet_init $DXNET_PATH

	# Kill any still running instances from previous deployments
	for i in $(seq 0 $((total_nodes - 1))); do
		cdepl_app_dxnet_node_cleanup $i
	done

	# Ring structure
	# One node sends to its successor and receives from its predecessor
	for i in $(seq 0 $((total_nodes - 2))); do
		cdepl_app_dxnet_node_send_targets $i $((i + 1))
	done

	# Last node sends to first
	cdepl_app_dxnet_node_send_targets $((total_nodes - 1)) 0

	# Set network type
	depl_app_dxnet_network $NETWORK_TYPE

	# Set further parameters for nodes
	for i in $(seq 0 $((total_nodes - 1))); do
		depl_app_dxnet_workload $i $workload
		depl_app_dxnet_msg_send_count $i $msg_per_node
		depl_app_dxnet_msg_recv_count $i $msg_per_node
		depl_app_dxnet_msg_size $i $msg_size

		cdepl_app_dxnet_node_send_threads $i $threads
		cdepl_app_dxnet_node_message_handler $i $msg_handler

		if [ "$NETWORK_TYPE" = "ib" ]; then
			cdepl_app_dxnet_run_as_sudo $i
		fi
	done

	# Start all instances
	cdepl_app_dxnet_start_node 0 $((total_nodes - 1))

	# Wait for all instances to finish, this also checks for runtime errors
	for i in $(seq 0 $((total_nodes - 1))); do
		cdepl_app_dxnet_node_wait_finished $i
	done

	# Print results
	for i in $(seq 0 $((total_nodes - 1))); do
		printf "######################\nResults node $i\n"
		cdepl_app_dxnet_node_get_results $i
		printf "######################\n"
	done

	# Kill any leftovers
	cdepl_app_dxnet_node_cleanup 0 $((total_nodes - 1))

	# Pack results and move
	cdepl_deploy_archive_out_path "dxnet_${workload}_${total_nodes}_${msg_size}_${threads}_${msg_handler}" ${RESULT_ARCHIVES_OUT_PATH}/${benchmark_name}

	# Reset output path for next benchmark call
	cdepl_deploy_out_path $OUT_PATH
}

##
# Run a one-to-all benchmark. A single send only node sends to all other
# receive only nodes.
#
# $1 Workload id (0, 1, 2, 3)
# $2 Total number of nodes to use for the benchmark
# $3 Number of messages to send to _each_ other node
# $4 Payload size for the messages to send
# $5 Number of application/send threads
# $6 Number of message handlers for receiving incoming messages
# $7 Name of the benchmark configuration (for grouping the archives of the
#    benchmark results)
##
benchmark_one_to_all()
{
	local workload=$1
	local total_nodes=$2
	local msg_per_node=$3
	local msg_size=$4
	local threads=$5
	local msg_handler=$6
	local benchmark_name=$7

	echo "##### One-to-all benchmark: $workload $total_nodes $msg_per_node $msg_size $threads $msg_handler"

	if [ "$total_nodes" -lt "2" ]; then
		util_log_error_and_exit "Benchmark needs at least two nodes, provided: $total_nodes"
	fi

	# Initialize dxnet deployment
	cdepl_app_dxnet_init $DXNET_PATH

	# Kill any still running instances from previous deployments
	cdepl_app_dxnet_node_cleanup 0 $((total_nodes - 1))

	# One-to-all
	# One node send only, other nodes recv only
	cdepl_app_dxnet_node_send_targets 0 "$(seq -s ' ' 1 $((total_nodes - 1)))"

	# Parameters sender
	depl_app_dxnet_msg_send_count 0 $((msg_per_node * (total_nodes - 1)))
	depl_app_dxnet_msg_recv_count 0 0
	cdepl_app_dxnet_node_send_threads 0 $threads

	# Sender might need message handlers for request-response
	cdepl_app_dxnet_node_message_handler 0 $msg_handler

	# Parameters receivers
	for i in $(seq 1 $((total_nodes - 1))); do
		depl_app_dxnet_msg_send_count $i 0
		depl_app_dxnet_msg_recv_count $i $msg_per_node

		# Receiver doesn't need send threads
		cdepl_app_dxnet_node_send_threads $i 0
		cdepl_app_dxnet_node_message_handler $i $msg_handler
	done

	# Set further parameters for all nodes
	for i in $(seq 0 $((total_nodes - 1))); do
		depl_app_dxnet_workload $i $workload
		depl_app_dxnet_msg_size $i $msg_size

		if [ "$NETWORK_TYPE" = "ib" ]; then
			cdepl_app_dxnet_run_as_sudo $i
		fi
	done

	# Set network type
	depl_app_dxnet_network $NETWORK_TYPE

	# Start all instances
	cdepl_app_dxnet_start_node 0 $((total_nodes - 1))

	# Wait for all instances to finish, this also checks for runtime errors
	for i in $(seq 0 $((total_nodes - 1))); do
		cdepl_app_dxnet_node_wait_finished $i
	done

	# Print results
	for i in $(seq 0 $((total_nodes - 1))); do
		printf "######################\nResults node $i\n"
		cdepl_app_dxnet_node_get_results $i
		printf "######################\n"
	done

	# Kill any leftovers
	cdepl_app_dxnet_node_cleanup 0 $((total_nodes - 1))

	# Pack results and move
	cdepl_deploy_archive_out_path "dxnet_${workload}_${total_nodes}_${msg_size}_${threads}_${msg_handler}" ${RESULT_ARCHIVES_OUT_PATH}/${benchmark_name}

	# Reset output path for next benchmark call
	cdepl_deploy_out_path $OUT_PATH
}

##
# Run a all-to-one benchmark. A single receiver node receives messages from
# all other send only nodes.
#
# $1 Workload id (0, 1, 2, 3)
# $2 Total number of nodes to use for the benchmark
# $3 Number of messages to send from each other node to single receiver
# $4 Payload size for the messages to send
# $5 Number of application/send threads
# $6 Number of message handlers for receiving incoming messages
# $7 Name of the benchmark configuration (for grouping the archives of the
#    benchmark results)
##
benchmark_all_to_one()
{
	local workload=$1
	local total_nodes=$2
	local msg_per_node=$3
	local msg_size=$4
	local threads=$5
	local msg_handler=$6
	local benchmark_name=$7

	echo "##### All-to-one benchmark: $workload $total_nodes $msg_per_node $msg_size $threads $msg_handler"

	if [ "$total_nodes" -lt "2" ]; then
		util_log_error_and_exit "Benchmark needs at least two nodes, provided: $total_nodes"
	fi

	# Initialize dxnet deployment
	cdepl_app_dxnet_init $DXNET_PATH

	# Kill any still running instances from previous deployments
	cdepl_app_dxnet_node_cleanup 0 $((total_nodes - 1))

	# All-to-one
	# One recv only, other nodes send only

	# Parameters receiver
	depl_app_dxnet_msg_send_count 0 0
	depl_app_dxnet_msg_recv_count 0 $((msg_per_node * (total_nodes - 1)))
	cdepl_app_dxnet_node_message_handler 0 $msg_handler
	# Receiver doesn't need send threads
	cdepl_app_dxnet_node_send_threads 0 0

	# Sender target
	for i in $(seq 1 $((total_nodes - 1))); do
		cdepl_app_dxnet_node_send_targets $i 0
	done

	# Parameters senders
	for i in $(seq 1 $((total_nodes - 1))); do
		depl_app_dxnet_msg_send_count $i $msg_per_node
		depl_app_dxnet_msg_recv_count $i 0

		# Receiver doesn't need send threads
		cdepl_app_dxnet_node_send_threads 0 0
		cdepl_app_dxnet_node_message_handler 0 $msg_handler
	done

	# Set further parameters for all nodes
	for i in $(seq 0 $((total_nodes - 1))); do
		depl_app_dxnet_workload $i $workload
		depl_app_dxnet_msg_size $i $msg_size

		if [ "$NETWORK_TYPE" = "ib" ]; then
			cdepl_app_dxnet_run_as_sudo $i
		fi
	done

	# Set network type
	depl_app_dxnet_network $NETWORK_TYPE

	# Start all instances
	cdepl_app_dxnet_start_node 0 $((total_nodes - 1))

	# Wait for all instances to finish, this also checks for runtime errors
	for i in $(seq 0 $((total_nodes - 1))); do
		cdepl_app_dxnet_node_wait_finished $i
	done

	# Print results
	for i in $(seq 0 $((total_nodes - 1))); do
		printf "######################\nResults node $i\n"
		cdepl_app_dxnet_node_get_results $i
		printf "######################\n"
	done

	# Kill any leftovers
	cdepl_app_dxnet_node_cleanup 0 $((total_nodes - 1))

	# Pack results and move
	cdepl_deploy_archive_out_path "dxnet_${workload}_${total_nodes}_${msg_size}_${threads}_${msg_handler}" ${RESULT_ARCHIVES_OUT_PATH}/${benchmark_name}

	# Reset output path for next benchmark call
	cdepl_deploy_out_path $OUT_PATH
}

##
# One-to-one uni-directional warm up to quickly check if the max throughput
# is ok on the target cluster before continuing with the other benchmarks
##
benchmark_00()
{
	local msg_count=10000000
	local size="16384"
	local thread="1"
	local msg_handler="2"

	echo "##### benchmark 00 (DEBUG/CHECK CLUSTER THROUGHPUT) ######"

	# Throughput
	benchmark_one_to_one 0 10000000 16384 $thread $msg_handler "benchmark_00"

	echo "##### benchmark 00 (DEBUG/CHECK CLUSTER LATENCY) ######"

	# Latency
	benchmark_one_to_one 2 2000000 1 $thread $msg_handler "benchmark_00"

	echo "##### benchmark 00 finished ######"
}

##
# One-to-one uni-directional base line for throughput and saturation of messages
# with increasing payload size (mainly used for debugging to check for
# differences with end-to-end full duplex)
##
benchmark_01()
{
	local msg_count=100000000
	local sizes="1 2 4 8 16 32 64 128 256 512 1024 2048 4096 8192 16384"
	local thread="1"
	local msg_handler="2"

	echo "##### benchmark 01 ######"

	for size in $sizes; do
		benchmark_one_to_one 0 $msg_count $size $thread $msg_handler "benchmark_01"
	done

	echo "##### benchmark 01 finished ######"
}

##
# End-to-end (full duplex) base line for throughput and saturation of messages
# with increasing payload size
##
benchmark_02()
{
	local node="2"
	local msg_count=100000000
	local sizes="1 2 4 8 16 32 64 128 256 512 1024 2048 4096 8192 16384"
	local thread="1"
	local msg_handler="2"

	echo "##### benchmark_02 ######"

	for size in $sizes; do
		benchmark_all_to_all 0 $node $msg_count $size $thread $msg_handler "benchmark_02"
	done

	echo "##### benchmark_02 finished ######"
}

##
# One-to-one uni-directional base line for throughput and latency with
# request-response pattern swith increasing payload size
##
benchmark_03()
{
	local msg_count=10000000
	local sizes="1 2 4 8 16 32 64 128 256 512 1024 2048 4096 8192 16384"
	local thread="1"
	local msg_handler="2"

	echo "##### benchmark_03 ######"

	for size in $sizes; do
		benchmark_one_to_one 2 $msg_count $size $thread $msg_handler "benchmark_03"
	done

	echo "##### benchmark_03 finished ######"
}

##
# End-to-end full duplex base line for throughput and latency with
# request-response pattern with increasing payload size
##
benchmark_04()
{
	local node="2"
	local msg_count=10000000
	local sizes="1 2 4 8 16 32 64 128 256 512 1024 2048 4096 8192 16384"
	local thread="1"
	local msg_handler="2"

	echo "##### benchmark_04 ######"

	for size in $sizes; do
		benchmark_all_to_all 2 $node $msg_count $size $thread $msg_handler "benchmark_04"
	done

	echo "##### benchmark_04 finished ######"
}

##
# End-to-end latency baseline for minimal payload request-response latency
# with increasing application thread and message handler count
##
benchmark_05()
{
	local node="2"
	local msg_count=10000000
	local sizes="1"
	local threads="1 2 4 8 16 32 64 128"
	local msg_handlers="2 4 8 16"

	echo "##### benchmark_05 ######"

	for msg_handler in $msg_handlers; do
		for thread in $threads; do
			benchmark_all_to_all 2 $node $msg_count $size $thread $msg_handler "benchmark_05"
		done
	done

	echo "##### benchmark_05 finished ######"
}

##
# Base line for throughput and saturation of messages with increasing node
# count and different payload sizes
##
benchmark_06()
{
	local nodes="2 4 8 16 24 32 40 48 56 64 72 80 88 96 104 112"
	local sizes="1 2 4 8 16 32 64 128 256 512 1024 2048 4096 8192 16384"
	local msg_count=10000000
	local thread="1"
	local msg_handler="2"

	echo "##### benchmark_06 ######"

	for size in $sizes; do
		for node in $nodes; do
			if [ "$node" -le "$TOTAL_NODES" ]; then
				benchmark_all_to_all 0 $node $msg_count $size $thread $msg_handler "benchmark_06"
			fi
		done
	done

	echo "##### benchmark_06 finished ######"
}

##
# Throughput and scalability with very small messages (16 byte) with increasing
# node count and different thread counts
##
benchmark_07()
{
	local nodes="6 2 4 8 16 24 32 40 48 56 64 72 80 88 96 104 112"
	local size="16"
	local msg_count=100000000
	local threads="1 2 4 8 16 32 64 128"
	local msg_handler="8"

	echo "##### benchmark_07 ######"

	for node in $nodes; do
		if [ "$node" -le "$TOTAL_NODES" ]; then
			for thread in $threads; do
				benchmark_all_to_all 0 $node $msg_count $size $thread $msg_handler "benchmark_07"
			done
		fi
	done

	echo "##### benchmark_07 finished ######"
}

##
# High request-response load with minimal payload to show latency/throughput
# with increasing node node and different thread counts
##
benchmark_08()
{
	local nodes="2 4 8 16 24 32 40 48 56 64 72 80 88 96 104 112"
	local size="1"
	local msg_count=100000000
	local threads="1 2 4 8 16 32 64 128"
	local msg_handler="8"

	echo "##### benchmark_08 ######"

	for node in $nodes; do
		if [ "$node" -le "$TOTAL_NODES" ]; then
			for thread in $threads; do
				benchmark_all_to_all 2 $node $msg_count $size $thread $msg_handler "benchmark_08"
			done
		fi
	done

	echo "##### benchmark_08 finished ######"
}

##
# Key-value storage/graph pattern, request-response with 64 byte payload to
# show latency/throughput with increasing node count on different thread counts
##
benchmark_09()
{
	local nodes="2 4 8 16 24 32 40 48 56 64 72 80 88 96 104 112"
	local size="64"
	local msg_count=100000000
	local threads="1 2 4 8 16 32 64 128"
	local msg_handler="8"

	echo "##### benchmark_09 ######"

	for node in $nodes; do
		if [ "$node" -le "$TOTAL_NODES" ]; then
			for thread in $threads; do
				benchmark_all_to_all 2 $node $msg_count $size $thread $msg_handler "benchmark_09"
			done
		fi
	done

	echo "##### benchmark_09 finished ######"
}

##
# End-to-end with many nodes forming a ring structure. Node sends to its
# successor and receives from its predecessor with different node counts
# and message sizes.
##
benchmark_10()
{
	local nodes="2 4 8 16 24 32 40 48 56 64 72 80 88 96 104 112"
	local sizes="1 2 4 8 16 32 64 128 256 512 1024 2048 4096 8192 16384"
	local msg_count=100000000
	local thread="1"
	local msg_handler="2"

	echo "##### benchmark_10 ######"

	for size in $sizes; do
		for node in $nodes; do
			if [ "$node" -le "$TOTAL_NODES" ]; then
				benchmark_ring 0 $node $msg_count $size $thread $msg_handler "benchmark_10"
			fi
		done
	done

	echo "##### benchmark_10 finished ######"
}

##
# All to one messages with increasing node and thread counts
##
benchmark_11()
{
	local nodes="2 4 8 16 24 32 40 48 56 64 72 80 88 96 104 112"
	local size="64"
	local msg_count=100000000
	local threads="1 2 4 8 16 32 64 128"
	local msg_handler="8"

	echo "##### benchmark_11 ######"

	for node in $nodes; do
		if [ "$node" -le "$TOTAL_NODES" ]; then
			for thread in $threads; do
				benchmark_all_to_one 0 $node $msg_count $size $thread $msg_handler "benchmark_11"
			done
		fi
	done

	echo "##### benchmark_11 finished ######"
}

##
# All to one request-response with increasing node and thread counts
##
benchmark_12()
{
	local nodes="2 4 8 16 24 32 40 48 56 64 72 80 88 96 104 112"
	local size="64"
	local msg_count=100000000
	local threads="1 2 4 8 16 32 64 128"
	local msg_handler="8"

	echo "##### benchmark_12 ######"

	for node in $nodes; do
		if [ "$node" -le "$TOTAL_NODES" ]; then
			for thread in $threads; do
				benchmark_all_to_one 2 $node $msg_count $size $thread $msg_handler "benchmark_12"
			done
		fi
	done

	echo "##### benchmark_12 finished ######"
}

##
# One to all messages with increasing node and thread counts
##
benchmark_13()
{
	local nodes="2 4 8 16 24 32 40 48 56 64 72 80 88 96 104 112"
	local size="64"
	local msg_count=100000000
	local threads="1 2 4 8 16 32 64 128"
	local msg_handler="8"

	echo "##### benchmark_13 ######"

	for node in $nodes; do
		if [ "$node" -le "$TOTAL_NODES" ]; then
			for thread in $threads; do
				benchmark_one_to_all 0 $node $msg_count $size $thread $msg_handler "benchmark_13"
			done
		fi
	done

	echo "##### benchmark_13 finished ######"
}

################################################################################

cdepl_script_process_cmd_args()
{
	local total_nodes="$1"
	local cluster_type="$2"
	local cluster_user="$3"
	local cluster_type_init_args="${@:4}"

	if [ ! "$total_nodes" ]; then
		util_log_error "Missing argument 'total_nodes'"
		util_log_error_and_exit "Usage: <total_nodes> <cluster_type> <cluster_user> [init args for cluster type...]"
	fi

	if [ ! "$cluster_type" ]; then
		util_log_error "Missing argument 'cluster_type'"
		util_log_error_and_exit "Usage: <total_nodes> <cluster_type> <cluster_user> [init args for cluster type...]"
	fi

	if [ ! "$cluster_user" ]; then
		util_log_error "Missing argument 'cluster_user'"
		util_log_error_and_exit "Usage: <total_nodes> <cluster_type> <cluster_user> [init args for cluster type...]"
	fi

	TOTAL_NODES="$total_nodes"
	CLUSTER_TYPE="$cluster_type"
	CLUSTER_USER="$cluster_user"
	CLUSTER_TYPE_INIT_ARGS="$cluster_type_init_args"

	OUT_PATH="/home/${CLUSTER_USER}/scratch"
	DXNET_PATH="/home/${CLUSTER_USER}/dxnet"

	RESULT_ARCHIVES_OUT_PATH="${OUT_PATH}/dxnet_bench_results"
}

cdepl_script_cluster_node_setup()
{
	# Set the log level to output debug info
	util_log_set_level "$UTIL_LOG_LEVEL_DEBUG"

	# Init the cluster environment to deploy to
	cdepl_cluster_init $CLUSTER_TYPE $CLUSTER_USER $CLUSTER_TYPE_INIT_ARGS

	# Load application modules of apps you want to deploy
	cdepl_cluster_app_load "dxnet"

	# Alloc total number of nodes for this deployment
	cdepl_cluster_node_alloc $TOTAL_NODES

	# Walltime for your deployment, might be ignored depending
	# on cluster environment selected
	cdepl_cluster_walltime "02:00:00"

	# Reserve all nodes exclusive (all resources available)
	for i in $(seq 0 $((TOTAL_NODES - 1))); do
		# DXNet is not optimized for multi socket setups and doesn't perform
		# well with them, limit to single socket
		cdepl_cluster_node_cpu_limit_single_socket $i
		cdepl_cluster_node_excl $i
	done

	# Set our output path for log files and configurations
	# for the applications deployed
	cdepl_deploy_out_path $OUT_PATH
}

cdepl_script_deploy()
{
	cdepl_cluster_file_system_cmd "mkdir -p $RESULT_ARCHIVES_OUT_PATH"

	# All benchmarks are executed here. If you aren't interested in all
	# benchmarks, just remove what you don't need

	# TODO benchmarks dedicated to loopback

	#benchmark_00
	#benchmark_01
	#benchmark_02
	#benchmark_03
	#benchmark_04
	#benchmark_05
	#benchmark_06
	benchmark_07
	benchmark_08
	benchmark_09
	benchmark_10
	benchmark_11
	benchmark_12
	benchmark_13

	# Done
	echo "Archived results can be found in $RESULT_ARCHIVES_OUT_PATH"
}

cdepl_script_cleanup()
{
	# Kill any leftovers
	cdepl_app_dxnet_node_cleanup 0 $((TOTAL_NODES - 1))
}